{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides examples for interacting with Zarr stores created by `clam collect`, using Dask.\n",
    "\n",
    "The Zarr store structure:\n",
    "- Root group with `clam_metadata` containing contigs, column_names (sample names), and chunk_size\n",
    "- One array per chromosome with shape `(chrom_length, n_samples)`\n",
    "- Arrays store u32 depth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "\n",
    "try:\n",
    "    client = Client(\"tcp://localhost:8786\", timeout=\"2s\")\n",
    "except OSError:\n",
    "    cluster = LocalCluster(scheduler_port=8786)\n",
    "    client = Client(cluster)\n",
    "client.restart()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "from typing import Any, cast\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ClamDepthStore:\n",
    "    def __init__(self, path: str | Path):\n",
    "        self.path = Path(path)\n",
    "\n",
    "        self.store = zarr.open(self.path, mode=\"r\")\n",
    "\n",
    "        metadata = cast(dict[str, Any], self.store.attrs[\"clam_metadata\"])\n",
    "        self.chunk_size: int = metadata[\"chunk_size\"]\n",
    "        self.sample_names: list[str] = metadata[\"column_names\"]\n",
    "        self.contigs: dict[str, int] = {\n",
    "            c[\"name\"]: c[\"length\"] for c in metadata[\"contigs\"]\n",
    "        }\n",
    "\n",
    "    def get_dask_array(self, contig: str) -> da.Array:\n",
    "        \"\"\"Get a dask array for a contig.\"\"\"\n",
    "        return da.from_zarr(self.path / contig)\n",
    "\n",
    "    def mean_depth_contig(self, contig: str) -> float:\n",
    "        \"\"\"Mean depth using dask.\"\"\"\n",
    "        return self.get_dask_array(contig).mean().compute()\n",
    "\n",
    "    def mean_depth_all_contigs(self) -> pd.Series:\n",
    "        \"\"\"Mean depth per contig, computed in parallel.\"\"\"\n",
    "\n",
    "        means = {contig: self.get_dask_array(contig).mean() for contig in self.contigs}\n",
    "\n",
    "        results = da.compute(means)[0]\n",
    "        return pd.Series(results)\n",
    "\n",
    "    def depth_statistics(self, contig: str) -> dict[str, float]:\n",
    "        \"\"\"Compute stats in single pass with dask.\"\"\"\n",
    "        arr = self.get_dask_array(contig)\n",
    "\n",
    "        stats = {\n",
    "            \"mean\": arr.mean(),\n",
    "            \"std\": arr.std(),\n",
    "            \"min\": arr.min(),\n",
    "            \"max\": arr.max(),\n",
    "        }\n",
    "\n",
    "        return da.compute(stats)[0]\n",
    "\n",
    "    def depth_statistics_all_contigs(self) -> pd.DataFrame:\n",
    "        \"\"\"Compute stats for all contigs in parallel.\"\"\"\n",
    "\n",
    "        all_stats = {}\n",
    "        for contig in self.contigs:\n",
    "            arr = self.get_dask_array(contig)\n",
    "            all_stats[contig] = {\n",
    "                \"mean\": arr.mean(),\n",
    "                \"std\": arr.std(),\n",
    "                \"min\": arr.min(),\n",
    "                \"max\": arr.max(),\n",
    "            }\n",
    "\n",
    "        # Execute all at once\n",
    "        results = da.compute(all_stats)[0]\n",
    "        return pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = ClamDepthStore(path=\"\")  # put your clam collect zarr path here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_stats = store.depth_statistics_all_contigs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_threshold_file(\n",
    "    depth_stats: pd.DataFrame, output_path: str | Path, n_std: float = 2.0\n",
    "):\n",
    "    \"\"\"Create threshold TSV from depth statistics using Poisson assumption.\"\"\"\n",
    "    mean = depth_stats[\"mean\"]\n",
    "    poisson_std = np.sqrt(mean)\n",
    "\n",
    "    thresholds = pd.DataFrame(\n",
    "        {\n",
    "            \"contig\": depth_stats.index,\n",
    "            \"min_depth\": (mean - n_std * poisson_std).clip(lower=0),\n",
    "            \"max_depth\": mean + n_std * poisson_std,\n",
    "        }\n",
    "    )\n",
    "    thresholds.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "make_threshold_file(depth_stats, \"thresholds.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
